<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>VAN-Flow</title>

  <style>
    body {
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Helvetica Neue", Arial, sans-serif;
      line-height: 1.6;
      margin: 0;
      background: #ffffff;
      color: #111;
    }

    .hero {
      text-align: center;
      margin-top: 60px;
      margin-bottom: 40px;
      padding: 0 16px;
    }

    .hero h1 {
      font-size: 44px;
      margin-bottom: 10px;
      letter-spacing: -0.4px;
    }

    .brand {
      font-weight: 800;
      color: #FF2D8E;
    }

    .buttons {
      margin-top: 18px;
    }

    .buttons a {
      display: inline-block;
      padding: 10px 18px;
      margin: 6px;
      border-radius: 10px;
      border: 1px solid #ddd;
      text-decoration: none;
      color: #111;
      font-weight: 600;
      background: #fff;
    }

    .buttons a:hover {
      background: #f3f3f3;
    }

    .section {
      max-width: 900px;
      margin: 0 auto 70px auto;
      padding: 0 20px;
    }

    .section h2 {
      margin-top: 0;
    }

    .figure {
      text-align: center;
      margin: 30px 0 10px 0;
    }

    .figure img {
      max-width: 100%;
      border-radius: 12px;
      border: 1px solid #eee;
    }

    .caption {
      text-align: center;
      font-size: 14px;
      color: #666;
      margin-top: 10px;
    }

    /* ===== MP4 Grid ===== */
/* ===== 4-Column Video Comparison Layout ===== */
    .video-section {
      max-width: 1300px;
      margin: 0 auto 70px auto;
      padding: 0 20px;
    }
    
    .video-title-row {
      display: grid;
      grid-template-columns: repeat(4, 1fr);
      text-align: center;
      font-weight: 700;
      margin-bottom: 10px;
    }
    
    .video-grid {
      display: grid;
      grid-template-columns: repeat(4, 1fr);
      gap: 20px;
      align-items: start;
    }
    
    .video-card {
      text-align: center;
    }
    
    .video-card p {
      margin: 6px 0 8px 0;
      font-size: 14px;
      font-weight: 600;
    }
    
    .tag-proposed { color: #FF2D8E; }
    .tag-baseline { color: #444; }
    
    .video-card video {
      width: 100%;
      border-radius: 12px;
      border: 1px solid #eee;
      background: #000;
    }
    
    @media (max-width: 1000px) {
      .video-grid, .video-title-row {
        grid-template-columns: repeat(2, 1fr);
      }
    }

  </style>

  <!-- MathJax -->
  <script>
    window.MathJax = {
      tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['\\[', '\\]']] },
      svg: { fontCache: 'global' }
    };
  </script>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
</head>

<body>

  <div class="hero">
    <h1>
      <span class="brand">VAN-Flow:</span>
      Variance-Averse $n$-Step Offline Reinforcement Learning <br>for Sparse Long-Horizon Environments
    </h1>
    <h2>ICML 2026</h2>
    <p><em>Anonymous Authors</em></p>

    <div class="buttons">
      <a href="paper.pdf">ðŸ“„ Paper</a>
      <a href="https://github.com/your-repo">ðŸ’» Code</a>
    </div>
  </div>

  <!-- ===== Qualitative Videos ===== -->
  <div class="video-section">
    <h2>Qualitative Results</h2>
    <div class="video-grid">
      <div class="video-card">
        <h3>AntMaze-Large-Play</h3>
        <p class="tag-baseline">FQL-n</p>
        <video autoplay loop muted playsinline>
          <source src="assets/video/antmaze_large_play_fql.mp4" type="video/mp4">
        </video>
      </div>

      <div class="video-card">
        <h3>AntMaze-Large-Play</h3>
        <p class="tag-proposed">Proposed (VAN-Flow)</p>
        <video autoplay loop muted playsinline>
          <source src="assets/video/antmaze_large_play_vanflow.mp4" type="video/mp4">
        </video>
      </div>

      <div class="video-card">
        <h3>AntMaze-Explore</h3>
        <p class="tag-baseline">FQL-n</p>
        <video autoplay loop muted playsinline>
          <source src="assets/video/antmaze_explore_fql.mp4" type="video/mp4">
        </video>
      </div>

      <div class="video-card">
        <h3>AntMaze-Explore</h3>
        <p class="tag-proposed">Proposed (VAN-Flow)</p>
        <video autoplay loop muted playsinline>
          <source src="assets/video/antmaze_explore_vanflow.mp4" type="video/mp4">
        </video>
      </div>
    </div>
  </div>

  <!-- ===== Abstract ===== -->
  <div class="section">
    <h2>Abstract</h2>
    <p>
      Offline reinforcement learning (RL) in sparse, long-horizon environments is challenging because value estimation
      errors compound over long backup horizons. While $n$-step returns mitigate long-horizon bias, they amplify return
      variance, destabilizing offline learning. We propose <b style="color:#FF2D8E;">VAN-Flow</b>, a variance-averse
      offline RL framework that combines multi-step learning with categorical distributional value estimation and
      variance-aware target weighting.
    </p>
  </div>

  <!-- ===== NEW SECTION ===== -->
  <div class="section">
    <h2>Why Variance-Averse Training is Needed?</h2>

    <div class="figure">
      <img src="assets/figures/why_variance_averse.pdf" alt="Why variance-averse training is needed">
    </div>
    <div class="caption">
      Performance comparison under increasing dataset return variance.
    </div>

    <p>
      In sparse, long-horizon offline RL, reducing the effective backup horizon using $n$-step returns is known to
      alleviate the <em>curse of horizon</em>. However, increasing the return horizon inevitably amplifies return
      variance. This issue becomes particularly severe in offline RL, where datasets are collected from diverse or
      noisy behavior policies, leading to highly heterogeneous return distributions.
    </p>

    <p>
      The figure above illustrates this trade-off. When dataset return variance is low (e.g., <b>navigate</b> or
      <b>play</b>), naively applying $n$-step returns (FQL-n) improves performance by shortening the backup horizon.
      However, as return variance increases (e.g., <b>explore</b> or <b>noisy</b>), the same naive $n$-step strategy
      becomes unstable and may even collapse. In contrast, <b style="color:#FF2D8E;">variance-averse training</b>
      maintains strong performance across both low-variance and high-variance regimes by downweighting unreliable,
      high-variance return targets.
    </p>

    <p>
      This observation shows that the effectiveness of $n$-step returns in offline RL is fundamentally determined by
      how return variance is controlled. Variance-averse learning enables us to preserve the bias-reduction benefits of
      long-horizon returns while preventing performance degradation caused by amplified variance.
    </p>
  </div>

  <div class="footer" style="max-width:900px;margin:0 auto 40px auto;padding:0 20px;color:#777;font-size:13px;text-align:center;">
    Â© Anonymous Authors â€¢ VAN-Flow Project Page
  </div>

</body>
</html>
