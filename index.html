<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>Variance-Averse \[n\]-Step Offline Reinforcement Learning for Sparse Long-Horizon Environments</title>
  <style>
    body {
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen;
      line-height: 1.6;
      margin: 0;
      background: #ffffff;
      color: #111;
    }

    .hero {
      text-align: center;
      margin-top: 60px;
      margin-bottom: 40px;
    }

    .hero h1 {
      font-size: 44px;
      margin-bottom: 10px;
    }

    .hero p {
      font-size: 18px;
      color: #555;
    }

    .buttons a {
      display: inline-block;
      padding: 10px 18px;
      margin: 6px;
      border-radius: 8px;
      border: 1px solid #ddd;
      text-decoration: none;
      color: #111;
      font-weight: 500;
    }

    .buttons a:hover {
      background: #f3f3f3;
    }

    .section {
      max-width: 900px;
      margin: 0 auto 70px auto;
      padding: 0 20px;
    }

    .figure {
      text-align: center;
      margin: 30px 0 10px 0;
    }

    .caption {
      text-align: center;
      font-size: 14px;
      color: #666;
      margin-bottom: 40px;
    }

    .highlight-box {
      background: #f7f7f7;
      border-radius: 10px;
      padding: 20px 25px;
      margin-top: 25px;
    }
  </style>
</head>

<body>

<div class="hero">
  <h1>Variance-Averse n-Step Offline Reinforcement Learning</h1>
  <p>Loss-Guided Distributional Value Estimation for Stable Long-Horizon Offline RL</p>
  <p><em>Anonymous Authors</em></p>

  <div class="buttons">
    <a href="#">üìÑ Paper</a>
    <a href="#">üíª Code</a>
    <a href="#">üåê Project Page</a>
  </div>
</div>

<div class="section">
  <h2>Abstract</h2>
  <p>
  Offline reinforcement learning (RL) in sparse, long-horizon environments is challenging because value estimation errors compound over long backup horizons, and offline datasets often provide limited coverage. While $n$-step returns mitigate long-horizon bias, they also amplify return variance, which can destabilize offline learning under heterogeneous behavior data. We show that this variance amplification constitutes a primary failure mode of na√Øvely applying long-horizon return targets in offline RL. To address this challenge, we propose \textbf{\color{magenta}VAN-Flow}, a variance-averse offline RL framework that couples $n$-step learning with categorical distributional value estimation. VAN-Flow introduces a variance-averse expectation that downweights unreliable high-variance return targets, together with a $Q$-guided flow-matching policy and rejection sampling to mitigate out-of-distribution actions. Notably, VAN-Flow shows remarkable improvements on challenging datasets with heterogeneous behavior data.
  </p>
</div>

<div class="section">
  <h2>Method Overview</h2>
  <div class="figure">
    <img src="assets/overview.png" width="90%">
  </div>
  <div class="caption">Figure 1. Overview of the proposed framework.</div>
</div>

<div class="section">
  <h2>Key Takeaways</h2>
  <div class="highlight-box">
    <b>Stable long-horizon learning</b><br>
    Controls variance amplification in multi-step offline RL.<br><br>

    <b>Robust under heterogeneous datasets</b><br>
    Maintains performance as return variance increases.<br><br>

    <b>Consistent improvements</b><br>
    Outperforms strong baselines on sparse long-horizon benchmarks.
  </div>
</div>

</body>
</html>
