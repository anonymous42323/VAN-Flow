<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>VAN-Flow</title>

  <style>
    body {
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Helvetica Neue", Arial, sans-serif;
      line-height: 1.6;
      margin: 0;
      background: #ffffff;
      color: #111;
    }

    .hero {
      text-align: center;
      margin-top: 60px;
      margin-bottom: 18px;
      padding: 0 16px;
    }

    .hero h1 {
      font-size: 44px;
      margin-bottom: 10px;
      letter-spacing: -0.4px;
    }

    .brand { font-weight: 800; color: #FF2D8E; }

    .buttons { margin-top: 18px; }

    .buttons a {
      display: inline-block;
      padding: 10px 18px;
      margin: 6px;
      border-radius: 10px;
      border: 1px solid #ddd;
      text-decoration: none;
      color: #111;
      font-weight: 600;
      background: #fff;
    }

    .buttons a:hover { background: #f3f3f3; }

    /* ===== Width alignment: ALL sections match video width ===== */
    .section,
    .intro-wrap,
    .video-section {
      max-width: 1300px;
      margin: 0 auto 70px auto;
      padding: 0 20px;
    }

    .figure { text-align: center; margin: 30px 0 10px 0; }
    .figure img {
      max-width: 100%;
      height: auto;
      border-radius: 12px;
      border: 1px solid #eee;
    }

    .caption {
      text-align: center;
      font-size: 14px;
      color: #666;
      margin-top: 10px;
    }

    /* ===== Fancy Section Title ===== */
    .fancy-title {
      text-align: center;
      font-size: 34px;
      font-weight: 800;
      margin: 80px 0 10px 0;
      letter-spacing: -0.5px;
      position: relative;
    }

    .fancy-title::after {
      content: "";
      display: block;
      width: 70px;
      height: 4px;
      margin: 12px auto 0 auto;
      border-radius: 2px;
      background: linear-gradient(90deg, #00c6a7, #00a8ff);
    }

    .section-divider {
      width: 100%;
      height: 1px;
      background: #e8e8e8;
      margin: 70px 0 40px 0;
    }

    /* ===== Intro Question Block ===== */
    .intro-wrap { margin-bottom: 36px; }

    .intro-lead{
      text-align: center;
      font-size: 16px;
      color: #444;
      margin: 0 0 14px 0;
    }

    .qbox{
      border: 1px solid #e9e9ee;
      border-radius: 16px;
      padding: 26px 22px;
      background: linear-gradient(180deg, #ffffff 0%, #fbfbff 100%);
      box-shadow: 0 10px 26px rgba(0,0,0,0.05);
    }

    .qtext{
      margin: 0;
      text-align: center;
      font-family: Georgia, "Times New Roman", Times, serif;
      font-style: italic;
      font-size: 30px;
      line-height: 1.25;
      letter-spacing: -0.3px;
      color: #111;
    }

    .highlight-box{
      margin-top: 16px;
      border-radius: 14px;
      padding: 14px 16px;
      background: rgba(255, 45, 142, 0.08);
      border: 1px solid rgba(255, 45, 142, 0.25);
      text-align: center;
      font-weight: 700;
      color: #111;
    }

    .highlight-box .emph{
      color: #FF2D8E;
      font-weight: 900;
    }

    .subhint{
      margin-top: 10px;
      text-align: center;
      font-size: 14px;
      color: #666;
    }

    /* ===== Qualitative Videos ===== */
    /* (4) Pair titles centered above (1,2) and (3,4) */
    .pair-header-row {
      display: grid;
      grid-template-columns: 1fr 1fr 78px 1fr 1fr; /* (3) 65% of 120px = 78px */
      align-items: end;
      margin-bottom: 10px;
    }

    .pair-title {
      grid-column: span 2;
      text-align: center;
      font-weight: 800;
      font-size: 16px;
      letter-spacing: -0.1px;
    }

    .mid-gap { width: 100%; }

    /* (2) separators between 1|2 and 3|4 + (3) reduced middle gap */
    .video-grid {
      display: grid;
      grid-template-columns: 1fr 22px 1fr 78px 1fr 22px 1fr;
      align-items: start;
    }

    .pair-sep {
      display: flex;
      align-items: center;
      justify-content: center;
      color: #cfcfcf;
      font-weight: 800;
      font-size: 18px;
      user-select: none;
      line-height: 1;
    }

    .pair-sep::before { content: "|"; }

    .video-card { text-align: center; }

    .video-card p {
      margin: 6px 0 8px 0;
      font-size: 14px;
      font-weight: 700;
    }

    .tag-proposed { color: #FF2D8E; }
    .tag-baseline { color: #444; }

    /* square videos */
    .video-frame {
      width: 100%;
      aspect-ratio: 1 / 1;
      border-radius: 12px;
      border: 1px solid #eee;
      overflow: hidden;
      background: #000;
    }

    .video-frame video {
      width: 100%;
      height: 100%;
      object-fit: cover;
      display: block;
      background: #000;
    }

    /* Mobile stacking */
    @media (max-width: 1000px) {
      .pair-header-row { grid-template-columns: 1fr; gap: 8px; }
      .pair-title { grid-column: auto; }
      .mid-gap, .pair-sep { display: none; }
      .video-grid { grid-template-columns: 1fr; gap: 18px; }
      .hero h1 { font-size: 34px; }
      .qtext { font-size: 22px; }
    }
  </style>

  <!-- MathJax -->
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['\\[', '\\]']],
        processEscapes: true
      },
      svg: { fontCache: 'global' }
    };
  </script>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
</head>

<body>

  <div class="hero">
    <h1>
      <span class="brand">VAN-Flow:</span>
      Variance-Averse $n$-Step Offline Reinforcement Learning <br>for Sparse Long-Horizon Environments
    </h1>
    <h2>ICML 2026</h2>
    <p><em>Anonymous Authors</em></p>

    <div class="buttons">
      <a href="paper.pdf">ðŸ“„ Paper</a>
      <a href="https://github.com/your-repo">ðŸ’» Code</a>
    </div>
  </div>

  <!-- ===== Opening: question + highlight + videos ===== -->
  <div class="intro-wrap">
    <p class="intro-lead">
      In this paper, we address a core challenge of offline RL in <b>long-horizon, sparse-reward</b> environments:
      leveraging <b>$n$-step returns</b> without suffering from variance amplification.
    </p>

    <div class="qbox">
      <p class="qtext">
        How can we leverage $n$-step returns in a variance-aware manner to improve offline RL performance in sparse,
        long-horizon environments?
      </p>

      <div class="highlight-box">
        Our answer: <span class="emph">variance-averse $n$-step returns</span> stabilize long-horizon learning by
        downweighting unreliable high-variance return targets.
      </div>

      <div class="subhint">
        Below, we provide qualitative comparisons (FQL-n vs. <span style="color:#FF2D8E;font-weight:800;">VAN-Flow</span>).
      </div>
    </div>
  </div>

  <div class="section-divider"></div>
  <h2 class="fancy-title">Qualitative Results</h2>

  <div class="video-section">
    <!-- Pair titles -->
    <div class="pair-header-row">
      <div class="pair-title">AntMaze-Play</div>
      <div class="mid-gap"></div>
      <div class="pair-title">AntMaze-Explore</div>
    </div>

    <div class="video-grid">
      <!-- 1 -->
      <div class="video-card">
        <p class="tag-baseline">FQL-n</p>
        <div class="video-frame">
          <video autoplay loop muted playsinline preload="metadata">
            <source src="assets/video/antmaze_play_fql.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </div>
      </div>

      <!-- sep 1|2 -->
      <div class="pair-sep" aria-hidden="true"></div>

      <!-- 2 -->
      <div class="video-card">
        <p class="tag-proposed">VAN-Flow (Ours)</p>
        <div class="video-frame">
          <video autoplay loop muted playsinline preload="metadata">
            <source src="assets/video/antmaze_play_vanflow.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </div>
      </div>

      <!-- middle gap (65%) -->
      <div class="mid-gap"></div>

      <!-- 3 -->
      <div class="video-card">
        <p class="tag-baseline">FQL-n</p>
        <div class="video-frame">
          <video autoplay loop muted playsinline preload="metadata">
            <source src="assets/video/antmaze_explore_fql.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </div>
      </div>

      <!-- sep 3|4 -->
      <div class="pair-sep" aria-hidden="true"></div>

      <!-- 4 -->
      <div class="video-card">
        <p class="tag-proposed">VAN-Flow (Ours)</p>
        <div class="video-frame">
          <video autoplay loop muted playsinline preload="metadata">
            <source src="assets/video/antmaze_explore_vanflow.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </div>
      </div>
    </div>

    <div class="caption">
      Qualitative comparison between FQL-n and the proposed variance-averse VAN-Flow.
    </div>
  </div>

  <div class="section-divider"></div>
  <h2 class="fancy-title">Abstract</h2>

  <div class="section">
    <p>
      Offline reinforcement learning (RL) in sparse, long-horizon environments is challenging because value estimation
      errors compound over long backup horizons. While $n$-step returns mitigate long-horizon bias, they amplify return
      variance, destabilizing offline learning. We propose <b style="color:#FF2D8E;">VAN-Flow</b>, a variance-averse
      offline RL framework that combines multi-step learning with categorical distributional value estimation and
      variance-aware target weighting.
    </p>
  </div>

  <div class="section-divider"></div>
  <h2 class="fancy-title">Why Variance-Averse Training is Needed?</h2>

  <div class="section">
    <div class="figure">
      <img src="assets/figures/why_variance_averse.png" alt="Why variance-averse training is needed">
    </div>
    <div class="caption">
      Offline RL performance on sparse-reward tasks under varying dataset return variance.
    </div>

    <p>
      Long-horizon decision-making under sparse rewards is notoriously challenging because informative feedback is
      delayed and rarely observed, making value estimation highly susceptible to error propagation. This issue becomes
      substantially more severe in offline RL, where learning is constrained to a fixed dataset with limited coverage.
      Prior work identifies that this difficulty is driven by the <em>curse of horizon</em>, where bootstrapping errors
      accumulate over long trajectories and degrade value accuracy. A common remedy is <em>value horizon reduction</em>,
      most notably $n$-step returns, which shorten the effective backup horizon.
    </p>

    <p>
      However, increasing the return horizon inevitably amplifies return variance, which can destabilize learning.
      In offline RL, this effect is further exacerbated because larger $n$ aggregates rewards collected under diverse
      behavior policies, yielding highly heterogeneous and high-variance return targets. As shown above, naively
      applying $n$-step returns (FQL-n) works well when dataset variance is low (<b>navigate</b>, <b>play</b>) but can
      become ineffective or even collapse under high-variance datasets (<b>explore</b>, <b>noisy</b>).
      In contrast, <span class="tag-proposed">variance-averse training</span> stabilizes learning by downweighting
      unreliable high-variance return targets, consistently recovering the benefits of long-horizon returns.
    </p>

    <p>
      These results suggest that the effectiveness of $n$-step returns in offline RL is fundamentally determined by
      how return variance is controlled. VAN-Flow explicitly models return uncertainty with a distributional critic,
      enabling variance-aware learning that preserves the bias-reduction benefits of $n$-step returns without suffering
      from variance amplification.
    </p>
  </div>

  <div class="footer" style="max-width:1300px;margin:0 auto 40px auto;padding:0 20px;color:#777;font-size:13px;text-align:center;">
    Â© Anonymous Authors â€¢ VAN-Flow Project Page
  </div>

</body>
</html>
